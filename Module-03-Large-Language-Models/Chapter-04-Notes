### Week 03, Session 04: OpenAI API, Embeddings, & Function Calling

---

# 1: Introduction to Word Embeddings
The session begins with a conceptual explanation of how LLMs (Large Language Models) understand text and images using "Embeddings".

- The Concept: The instructor uses an analogy of a child deciding where to put an apple (Kitchen vs. Bedroom vs. Hall) based on features like "Is it eatable?" or "Do parents like it?". Models do the same by breaking down data into numerical vectors (e.g., [0.8, 0.9, 0.2]).

- Vector Math: To determine how closely related two concepts are, models compare their vectors.

  - Euclidean Distance: Measuring the straight-line distance between two points. Smaller distance = more similar.

  - Cosine Similarity: Measuring the angle between two vectors. Higher cosine value (closer to 1 or smaller angle) = more similar.

- Multimodal Embeddings: Advanced APIs (like Jina AI) can take both text and images and map them into the same dimensional vector space (e.g., 1024 dimensions). This allows you to mathematically compare a text string directly against an image.

# 2: Making OpenAI API Calls (Chatbot with History)
The instructor demonstrates how to interact with the OpenAI API using Python's httpx (or requests) library rather than the official openai SDK. This helps students understand the raw HTTP requests.

- API Anatomy: A standard API call requires:

  - URL: The endpoint (e.g., v1/chat/completions). For this course, a proxy URL (AI Pipe) is used.

  - Headers: Contains the Authorization (Bearer Token/API Key) and Content-Type.

  - JSON Payload (Data): Contains the model (e.g., gpt-4o-mini) and the messages.

- Managing State (Memory): LLMs are stateless; they have no memory of past turns. To build a chatbot, the developer must maintain a messages list that appends every user prompt and AI response, sending the entire history back to the API on every new request.

- Roles: * developer / system: Defines the AI's behavior/persona.

  - user: The input from the human.

  - assistant: The response generated by the AI.

# 3: Handling Images with Base64 Encoding
To pass local images to an LLM via an API, the image must be converted into text.

- Base64 Encoding: A standard method to encode binary data (like a .png file) into an ASCII string. The instructor demonstrates reading an image file in binary mode (rb), encoding it using Python's base64 library, and then decoding it back to an image to prove no data is lost.

- Sending to API: The encoded Base64 string is formatted into a Data URI (data:image/png;base64,...) and passed inside the content array of the user's message payload.

# 4: Function Calling (Structured Outputs)
A major challenge with LLMs is getting them to return data in a predictable, parseable format (like JSON) rather than conversational text.

- Function Calling / Tool Use: You can pass a "schema" (a JSON blueprint) to the API detailing a mock function and its parameters.

- Implementation: The instructor uses OpenAI's documentation to generate a schema for extracting product details (Manufacturing Date, Expiry Date, Product Name) from an image.

- Result: Instead of replying with "The product is manufactured in...", the API returns a structured JSON object conforming exactly to the requested schema, making it easy to parse and store in a database or Excel sheet.
